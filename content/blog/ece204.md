+++
title="ECE204 Numerical Methods"
date=2024-02-08
+++

These are my annotations for ECE204 _Numerical Methods_, Winter 2024, taught by Professor Douglas Harder.
Notes directly from the course are presented as text, while annotations are in blockquotes.

The course is an introduction to how computational methods may be used to solve engineering problems numerically.
Lectures give background on the techniques while labs use MATLAB and C++.

**Main takeaways:**

- "The real world is modeled through equations" that must be computed to solve problems.
- Solutions must be tested for robustness (stability) and have tolerable error, runtime.
- Mathematical descriptions (algorithms) should be used whenever possible for ability to verify the design and simulate conditions.
- The storage formats of computers (IEEE-754 floating point, UINT_MAX, big number operations) necessitate the use of specialized computer algorithms as opposed to more straightforward solving of problems.


# Prerequisites

- 
- A strong understanding of linear algebra.
- Familiarity with the C++ standard library.


# The six tools of numerical analysis

These six tools allow us to approximate solutions and run simulations within the constraints of floating-point number representations:

1. Weighted averages
2. Iteration
3. Linear algebra
4. Interpolation
5. Taylor series
6. Bracketing
7. Intermediate-value theorem


## Weighted averages

Suppose you have $n$ values $x_1, \ldots, x_n$, all of which approximate one value $x$.
The average of these $n$ values is

$$\bar{x} := \frac{1}{n} \sum_{j=1}^n x_j$$

If we have $n$ coefficients $c_1, \ldots, c_n$ with the property $\sum_{k=1}^n c_k = 1$, then we say that

$$\bar{x}_W := \sum{i=1}^n c_n x_n$$

is the weighted average of the values $x_1$ through $x_n$.

In some cases, a weighted average gives a better approximation than a simple average.
For example, a three-point sine approximation with the midpoint double-weighted is far better than an average of the three points.

$$\frac{1}4 \sin{2} + \frac{1}{2}\sin{2.5} + \frac{1}4 \sin{3}$$

This particular approximation is known as the _composite trapezoidal rule_.


## Fixed-point iteration

{% definition(ref="Fixed-point iteration") %}
_Fixed-point iteration_ is the process of performing an operation on the results of an input over and over again such that it converges to a value.
{% end %}

Fixed-point iteration is useful for problems that can be expressed in terms of a function $f: \mathbb{A}\mapsto\mathbb{B}$ and an input $x$ such that there exists a value

$$x = f(x)$$

If repeated recursive evaluation of the form

$$x_{k+1} = f(x_k)$$

has a suitable radius of convergence for our problem, then this can be used as a numerical way to find solutions to certain problems, like:

- the root of a polynomial (Newton's Method)
- the solutions to a linear system (the Jacobi Method)

{% example(ref="Fixed-point iteration function, C++") %}
We can use a function reference to implement a general routine that solves fixed-point functions.
```cpp
double fixed_point(double f(double), double x_guess) {
    double x_previous{ 0 };

    do {
        x_previous = x_guess;
        x_guess = f(x_guess);
    } while (x_previous != x_guess); // Convergence condition

    return x_guess;
}
```

Because the floating-point representation means `x_previous` and `x_guess` may not be exactly equal, we can amend this to use a _step tolerance_.
The _step tolerance_ is the closest difference between `x_previous` and `x_guess` that we will accept for them to be considered equal.

Further, note that if the function does not converge we need a way for the function to exit the loop.
Therefore, we also add a maximum number of iterations.
```cpp
double fixed_point(double f(double), double x_guess, double step_tolerance,
                   std::size_t max_iterations) {
    double x_previous{ 0 };

    for (std::size_t i{0}; i < max_iterations; ++i) {
        x_previous = x_guess;
        x_guess = f(x_guess);

        if (std::abs(x_previous - x_guess) >= step_tolerance) {
            return x_guess;
        }
    }
    throw std::runtime_error("fixed_point did not converge");
}
```
{% end %}


## Linear algebra

Many numerical solutions can be found by finding solutions to a system of linear equations.


## Interpolation

Given $n$ points of form $(x_k, y_k)$, there is a polynomial of degree $n-1$ that passes through all of them as long as all $x_k$ are unique.

The coefficients of the polynomial can be found by solving a system of linear equations.
The coefficient matrix of this operation is known as the _Vandermonde matrix_ and raises the inputted $x$ coordinates to the 0th - $n-1$th powers.

$$V(x_1, x_2, \ldots, x_n) = \begin{bmatrix} x_1^{n-1} & x_1^{n-2} & \ldots & x_1 & 1 \\\\ x_2^{n-1} & x_2^{n-2} & \ldots & x_2 & 1 \\\\ \vdots & \vdots & \ddots & \vdots & \vdots \\\\ x_{n-1}^{n-1} & x_{n-1}^{n-2} & \ldots & x_{n-1} & 1 \\\\ x_n^{n-1} & x_n^{n-2} & \ldots & x_n & 1\end{bmatrix}$$

The coefficients are found by solving the below system: 

$$\begin{bmatrix}a_{n-1} \\\\ a_{n-2} \\\\ \vdots \\\\ a_1 \\\\ a_0 \end{bmatrix} V = \begin{bmatrix} y_1 \\\\ y_2 \\\\ \vdots \\\\ y_{n-1} \\\\ y_n \end{bmatrix}$$

This makes sense because the interpolating polynomial for the $n=3$ (three-point) case must pass through each of the three, such that:

- $y_1 = ax_1^2 + bx_1 + c$
- $y_2 = ax_2^2 + bx_2 + c$
- $y_3 = ax_3^2 + bx_3 + c$

{% theorem(ref="Existence and uniqueness of interpolation polynomials") %}
Given $n+1$ points $(x_i, y_i)^n_{i=0}$ with distinct $x$ values.
Then there is one and only one polynomial $p_n(x)\in\mathbb{P}_n$ satisfying the condition [^AnneKvaerno]

$$p_n(x_i) = y_i,\quad i\in\\{0,\ldots,n\\}$$
{% end %}


# A definition of error

{% definition(ref="Error") %}
If $x_\text{approx}$ is an approximation of $x$, the error $\epsilon$ is such that

$$x = x_\text{approx} + \epsilon$$

The _absolute error_ is defined as

$$\epsilon_\text{abs} := \lvert x - x_\text{approx}\rvert$$

The relative error is given as:

$$\epsilon_\text{rel} := {\epsilon_\text{abs} \over \lvert x \rvert}$$
{% end %}

The relative error is preferred because it is unit-free.
For example, $\epsilon_\text{rel} = 0.01$ means the approximation is good to one part in a hundred regardless of the magnitude of the exact value.

A solution is only useful if its _maximum error_ is acceptable.
The _maximum absolute error_ of each of the algorithms below can be determined.
And should be, when evaluating potential solutions!

{% definition(ref="Significant digits") %}
We say an approximation $x_\text{approx}$ is _to (or within) $d$ significant digits_ if its relative error $\epsilon_\text{rel}$ satisfies the inequality:

$$\epsilon_\text{rel} \le 5\times 10^{-d}$$
{% end %}


## Rounding

Computers cannot store real numbers with infinite precision.
To compute results, algorithms must therefore round values during operations.

We round to $N$ digits by looking at the value of the $N+1$th digit.
If the digit is $>5$ we round up.
If the digit is $<5$ we round down.
If the digit is $5$, we look at the $N$th digit.
If it is even, round up.
Otherwise, round down.

This is so that there is no biasing of rounding up or down for that specific case.


## Precision and accuracy

Two algorithms can be compared based on their _precision_ versus _accuracy_.

{% definition(ref="Precision") %}
An algorithm is more precise if given uniformly-random inputs, it produces output that is has less variance.
{% end %}

{% definition(ref="Accuracy") %}
An algorithm is more accurate if given uniformly-random inputs, it produces output that has less deviance from the expected outputs
{% end %}


# Sources of error

## Subtractive cancellation

Subtractive cancellation is where subtracting good approximations to two nearby numbers yields a _far worse approximation_ than you might expect. [^WikipediaCatastrophic]

Cancellation is inherent to the subtraction operation: this means that even if the subtraction is computed exactly, if the inputs are approximated than the resulting error has subtractive cancellation.

{% theorem(ref="Subtractive cancellation") %}
_Subtractive cancellation_ occurs because subtraction is _ill-conditioned_ at nearby inputs.

Let $\bar{x}= x + \epsilon_x \cdot x$ and $\bar{y}= y + \epsilon_y\cdot y$ be approximations of $x$ and $y$ with relative errors $\epsilon_x, \epsilon_y$.

Then

$$\begin{align}\bar{x} - \bar{y} &= (x + \epsilon_x x) - (y + \epsilon_y y) \\\\ &= x - y + \epsilon_x x - \epsilon_y y \\\\ &= (x-y)\left (1 + \frac{x \epsilon_x - y \epsilon_y}{x - y}\right )\end{align}$$

Therefore the relative error of the **exact compuation** of approximate values is

$$\epsilon_{x-y} = \left\lvert \frac{x\epsilon_x - y\epsilon_y}{x-y}\right\rvert$$

Given the $x-y$ in the denominator, that means that $\epsilon_{x-y}\to\infty$ as $x-y \to 0$.
{% end %}

This is a problem when $x$ and $y$ are very close to each other.


## Floating-point error

The **Sterbenz lemma** is a theorem that states when floating-point differences are computed exactly.

{% theorem(ref="Sterbenz lemma") %}
In a floating-point number system with subnormal numbers (extended specification of IEEE 754), if $x$ and $y$ are floating point numbers such that

$$\frac{y}2 \le x \le 2y$$

then $x-y$ is also a floating-point number. Thus, a correctly rounded floating-point subtraction

$$x \ominus y = \text{fl}(x - y) = x - y$$

is computed exactly. [^WikipediaSterbenz]
{% end %}

Contrast the Sterbenz lemma to the phenomenon of subtractive cancellation.
Even if the lemma guarantees that the subtraction will be performed exactly, there may still be inherent error in the subtraction that results if the floating-point operands are themselves approximations!


# Condition number of a matrix

The _condition number_ of a matrix measures how sensitive it is to change.
It is defined using the matrix norm.

## Matrix norm

The matrix norm is defined as _controlling the growth_ when multiplied. [^MitIla]

{% definition(ref="Matrix norm") %}
The _norm_ of $A$ is the largest ratio

$$\lvert\lvert A \rvert\rvert := \text{max}_{x\ne 0} \frac{\lvert \lvert Ax\rvert\rvert}{\lvert\lvert x\rvert\rvert}$$

Through derivation this is equal to the square root of the largest eigenvalue of $A^T A$

$$\lvert\lvert A \rvert\rvert = \text{max}_{x\ne 0} \frac{x^T A^T Ax}{x^T x} = \Lambda (A^T A)$$
{% end %}


## Sensitivity to error

A matrix's sensitivity to error is measured by its **condition number**.



# Partial pivoting: solving linear systems

**Motivation:** Many physical phenomena can be modelled as systems of linear equations.
Further, many non-linear systems may be approximated using linear systems.

> Indeed, in 1947 John von Neumann and H. H. Goldstine published a comprehensive treatise on the presence of essentially random error, and how to minimize that error, while solving linear systems in their work _Numerical Inverting of Matrices of High Order_.

_Gaussian elimination_ is a common technique for solving such a system.
Given a system of $n$ linear equations in $n$ unknowns expressed as the coefficient matrix $A$, the variable vector $\vec u$ and the solution vector $\vec v$:

$$A\vec u = \vec v$$

we create the augmented matrix $A\lvert\vec v$ and apply the _elementary row operations_ on the matrix to reach row-echelon form. Backward substitution may be used to find the unique solution if $\text{rank}(A)=n$

{% definition(ref="Elementary row operations") %}
- Swapping two rows
- Adding a multiple of one row onto another
- Multiplying a row by a non-zero scalar
{% end %}

However, this presents several issues:

- Given dense $A$, the number of floating point operations may be as high as $\frac{2}3n^3 - \frac{1}2n^2 - \frac{1}6n$.
- Adding a multiple of one row to anther may be a source of subtractive cancellation
- Adding a large multiple of one row onto another row may give $x + y = y$ (ie swamp out the smaller value).

The _partial pivoting method_ maintains precision by saying to swap the row with the largest in magnitude entry on or below the pivot to the pivot row.

If the largest entry in absolute value is moved to the pivot, then when we add a multiple of one row onto another, we are guaranteed that

$$\lvert \frac{a_{k,j}}{a_{i,j}}\rvert \le 1$$

And avoid the issue of adding a significant multiple of one row onto another.


# Jacobi method

{% theorem(ref="Jacobi method") %}
The _Jacobi method_ is a **fixed-point theorem**.
It states that given a linear system of form

$$A \vec u = \vec v$$

with strictly diagonally-dominant matrix $A$ and known $\vec v$, then the recursive iteration

$$u_k = A_\text{diag}^{-1}(v - A_\text{off}\cdot u_{k-1})$$

always converges to the solution of the system $u$ given **any** initial guess $u_0$.
{% end %}


# Approximating integrals

There are three main methods:

1. **Midpoint rule**: estimate definite integral using _piecewise constant functions_.
2. **Trapezoidal rule**: estimate using _piecewise linear functions_.
3. **Simpson's rule**: estimate using _piecewise quadratic functions_.


## Trapezoid rule

{% definition(ref="Trapezoid rule") %}
Approximates the definite integral.

Given a function $f(x)$, we take a selection of points.
We then connect them into a set of trapezoids and calculate the area.

$$\int_a^b f(x)\, dx \approx (b-a)\frac{1}2(f(a)+f(b))$$

Given equal spacing of the points $$\Delta x$$ from $x_0$ to $x_{N-1}$, this becomes[^1]

$$\int_a^b f(x)\, dx \approx \frac{\Delta x}2 \sum^N_{k=1} (f(x_{k-1}) + f(x_k))$$
{% end %}

Comparisons can be drawn between this and Riemann summing.


## Centered $O(h^5)$ backwards approximation

We may integrate the interpolating polynomial on the points

- $(t_{k-3}, f(t_{k-3}))$
- $(t_{k-2}, f(t_{k-2}))$
- $(t_{k-1}, f(t_{k-1}))$
- $(t_k, f(t_k))$

For each $k$.
The interpolating polynomial will 


## Simpson's rule

{% definition(ref="Simpson's rule") %}
Approximate the function $f$ using piecewise quadratic functions.

- Partition the interval into even number of subintervals with same length.
- Over the first pair of subintervals approximate $\int_{x_0}^{x_2} f(x)\, dx$ with $\int_{x_0}^{x_2} p(x)\, dx$, where $p(x) = Ax^2 + Bx^2 + C$ is the interpolating polynomial passing through

- $(x_0, f(x_0))$
- $(x_1, f(x_1))$
- $(x_2, f(x_2))$

{% end %}

{% theorem(ref="Simpson's rule") %}
Assume that $f(x)$ is continuous over $I = [a, b]$. Let $n$ be a positive even integer and $\Delta x = \frac{b-a}n$. Let $I$ be divided into $n$ subintervals, each of length $\Delta x$, with endpoints at $P=\{x_0, x_1, x_2, \ldots, x_n \}$. Set

$$S_n = \frac{\Delta x}{3} \left [ f(x_0) + 4f(x_1) + 2 f(x_2) + \ldots + 2f(x_{n-2}) + 4f(x_{n-1}) + f(x_n) \right ]$$

Then:

$$\lim_{n\to +\infty} S_n = \int_a^b f(x)\, dx$$
{% end %}

The **error bound** for the statement above approximating $f(x)$ can be given as:

If $M$ is the maximum value of $\lvert f^{(4)}(x)\rvert$ over $I$, then the upper bound for the actual error $\delta$ in using $S_n$ to estimate $\int_a^bf(x)\, dx$ is given by

$$\delta \le \frac{M(b-a)^5}{180 n^4}$$


# Least-squares best-fitting polynomials

Suppose that we are given a linear system that is overdetermined.
There is no linear combination of the columns of $A$ that equals the target vetor.

What linear combination is _closest_ to $\vec v$?

Let $A: \mathcal{U}\mapsto \mathcal{V}$ be a linear map, usually from $\mathbb{R}^n\mapsto \mathbb{R}^m$

We want to minimize the norm:

$$\lvert\lvert \mathbb{A} \vec u - \vec v\rvert \rvert_{2}$$

and find a $\vec u$ that makes it as small as possible.

The object is to find $\vec u_0$ such that $\mathbb{A}u_0 - v$ is perpendicular to all vectors in range of $\mathbb{A}\vec u$

We derive that

$$u_0 = (\mathbb{A}^T\mathbb{A})^{-1} \mathbb{A}^T \vec v$$

$$A^T A \vec u_0 = A^T v$$

Is the vector that minimizes the norm.


# References

[^1]: Wikipedia Contributors, _Trapezoidal rule_, Accessed 2024.

[^WikipediaCatastrophic]: Wikipedia Contributors, _Catastrophic cancellation_, Accessed 2024-02-12, [[Online]](https://en.wikipedia.org/wiki/Catastrophic_cancellation)

[^WikipediaSterbenz]: Wikipedia Contributors, _Sterbenz lemma_, Accessed 2024-02-12, [[Online]](https://en.wikipedia.org/wiki/Sterbenz_lemma)

[^2]: LibreTexts Mathematics, _7.7: Approximate Integration_, Accessed 2024.

[^3]: Douglas Wilhelm Harder, _In a nutshell: Least-squares best-fitting polynomial_, [[PDF]](https://ece.uwaterloo.ca/~dwharder/nm/Lecture_materials/pdfs/5.3%20Least-squares%20best-fitting%20polynomials%20in%20a%20nutshell.pdf)

[^4]: Douglas Wilhelm Harder, _ECE 204 Numerical methods_, "Course introduction", [[PDF]](https://ece.uwaterloo.ca/~dwharder/nm/Lecture_materials/pdfs/1%20Course%20introduction.pdf)

[^DouglasHarder]: Douglas Wilhelm Harder, _ECE 204 Numerical methods_, "3. Tools", [[PDF]](https://ece.uwaterloo.ca/~dwharder/nm/Lecture_materials/pdfs/3.%20Tools.pdf)

[^5]: Oleksandr Kaleniuk, _Geometry for Programmers_, Published 2023, [[Book]](https://www.manning.com/books/geometry-for-programmers)

[^AnneKvaerno]: Anne Kværnø, _Lecture notes for TMA4320 Introduction to Scientific Computation_, "Polynomial interpolation", Published 2022-02-11, [[PDF]](https://www.math.ntnu.no/emner/TMA4320/2022v/pdf/PolynomialInterpolation.pdf)

[^MitIla]: Gilbert Strang, _Introduction to Linear Algebra_, Published 2023, [[Online]](https://math.mit.edu/~gs/linearalgebra/ila6/indexila6.html)
